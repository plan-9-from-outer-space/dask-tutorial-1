{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a81512fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bbaf545",
   "metadata": {},
   "source": [
    "<img src=\"https://raw.githubusercontent.com/dask/dask/main/docs/source/images/dask_horizontal.svg\"\n",
    "     width=\"60%\"\n",
    "     alt=\"Dask logo\\\" />\n",
    "\n",
    "# Parallel and Distributed Machine Learning\n",
    "So far we have seen how Dask makes data analysis scalable with parallelization via Dask DataFrames and Dask Array. Let's now see how [Dask-ML](https://ml.dask.org/) allows us to do machine learning in a parallel and distributed manner. Note, machine learning is really just a special case of data analysis (one that automates analytical model building), so the ðŸ’ª Dask gains ðŸ’ª we've seen will apply here as well!\n",
    "\n",
    "> If you'd like a refresher on the difference between parallel and distributed computing, [here's a good discussion on StackExchange](https://cs.stackexchange.com/questions/1580/distributed-vs-parallel-computing). You can also check out [The Beginner's Guide to Distributed Computing](https://towardsdatascience.com/the-beginners-guide-to-distributed-computing-6d6833796318).\n",
    "\n",
    "### What we'll cover:\n",
    "1. Types of scaling problems in ML\n",
    "2. Scale Scikit-Learn with Joblib+Dask (compute-bound)\n",
    "3. Scale Scikit-Learn with Dask-ML (memory-bound)\n",
    "4. Scale XGBoost with Dask"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e3c11a4",
   "metadata": {},
   "source": [
    "## Types of scaling problems in machine learning\n",
    "\n",
    "There are two main types of scaling challenges you can run into in your machine learning workflow: scaling the **size of your data** and scaling the **size of your model**. That is:\n",
    "\n",
    "1. **Memory-bound problems**: Data is larger than RAM, and sampling isn't an option.\n",
    "2. **CPU-bound problems**: Data fits in RAM, but training takes too long. Many hyperparameter combinations, a large ensemble of many models, etc.\n",
    "\n",
    "Here's a handy diagram for visualizing these problems:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6f1145d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "Image(url=\"images/dask-zones.png\", width=400)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4572e129",
   "metadata": {},
   "source": [
    "In the bottom-left quadrant, your datasets are not too large (they fit comfortably in RAM) and your model is not too large either. When these conditions are met, you are much better off using something like scikit-learn, XGBoost, and similar libraries. You don't need to leverage multiple machines in a distributed manner with a library like Dask-ML. However, if you are in any of the other quadrants, distributed machine learning is the way to go.\n",
    "\n",
    "Summarizing: \n",
    "\n",
    "* For in-memory problems, just use scikit-learn (or your favorite ML library).\n",
    "* For large models, use `dask` and `joblib` together with your favorite scikit-learn estimator.\n",
    "* For large datasets, use `dask_ml` or `dask-xgboost` estimators."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6571f76",
   "metadata": {},
   "source": [
    "## Scikit-Learn Refresher\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/coiled/data-science-at-scale/master/images/scikit_learn_logo_small.svg\" \n",
    "     width=\"30%\"\n",
    "     alt=\"sklearn logo\\\" />\n",
    "\n",
    "In this section, we'll quickly run through a typical Scikit-Learn workflow:\n",
    "\n",
    "* Load some data (in this case, we'll generate it)\n",
    "* Import the Scikit-Learn module for our chosen ML algorithm\n",
    "* Create an estimator for that algorithm and fit it with our data\n",
    "* Inspect the learned attributes\n",
    "* Check the accuracy of our model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "800cf0f5",
   "metadata": {},
   "source": [
    "### Generate some random data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea55b66a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_classification\n",
    "\n",
    "# Generate data\n",
    "X, y = make_classification(n_samples=10000, n_features=4, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18a0577b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's take a look at X\n",
    "X[:8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06d00f43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's take a look at y\n",
    "y[:8]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4328bcf5",
   "metadata": {},
   "source": [
    "### Fitting a SVC\n",
    "\n",
    "For this example, we will fit a [Support Vector Classifier](https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84aca93d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "estimator = SVC(random_state=0)\n",
    "estimator.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8e4d61c",
   "metadata": {},
   "source": [
    "We can inspect the learned features by taking a look a the `support_vectors_`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51398313",
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator.support_vectors_[:4]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71e11c69",
   "metadata": {},
   "source": [
    "And we check the accuracy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2ab53c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator.score(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c655296b",
   "metadata": {},
   "source": [
    "### Hyperparameter Optimization\n",
    "\n",
    "There are a few ways to learn the best *hyper*parameters while training. One is `GridSearchCV`.\n",
    "As the name implies, this does a brute-force search over a grid of hyperparameter combinations. Scikit-learn provides tools to automatically find the best parameter combinations via cross-validation (which is the \"CV\" in `GridSearchCV`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "619a7741",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e73bfac5",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "estimator = SVC(gamma='auto', random_state=0, probability=True)\n",
    "param_grid = {\n",
    "    'C': [0.001, 10.0],\n",
    "    'kernel': ['rbf', 'poly'],\n",
    "}\n",
    "\n",
    "# Brute-force search over a grid of hyperparameter combinations\n",
    "grid_search = GridSearchCV(estimator, param_grid, verbose=2, cv=2)\n",
    "grid_search.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39201c6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search.best_params_, grid_search.best_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ba9f067",
   "metadata": {},
   "source": [
    "## Compute Bound: Single-machine parallelism with Joblib\n",
    "With Joblib, we can say that Scikit-Learn has *single-machine* parallelism.\n",
    "\n",
    "**Any Scikit-Learn estimator that can operate in parallel exposes an `n_jobs` keyword**, which tells you how many tasks to run in parallel. Specifying `n_jobs=-1` jobs means running the maximum possible number of tasks in parallel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be7a0201",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "grid_search = GridSearchCV(estimator, param_grid, verbose=2, cv=2, n_jobs=-1)\n",
    "grid_search.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfd9d5ec",
   "metadata": {},
   "source": [
    "Notice that the computation above it is faster than before. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "903b2750",
   "metadata": {},
   "source": [
    "## Compute Bound: Multi-machine parallelism with Dask\n",
    "\n",
    "In this section we'll see how Dask (plus Joblib and Scikit-Learn) gives us multi-machine parallelism. Here's what our grid search graph would look like if we allowed Dask to schedule our training \"jobs\" over multiple machines in our cluster:\n",
    "\n",
    "Dask can talk to Scikit-Learn (via Joblib) so that our *Dask cluster* is used to train a model. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e089cf6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask.distributed import Client\n",
    "\n",
    "# create local Dask cluster with 8 workers (cores)\n",
    "client = Client(n_workers=8)\n",
    "client"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9f29710",
   "metadata": {},
   "source": [
    "**Note:** Click on Cluster Info, to see more details about the cluster. You can see the configuration of the cluster and some other specs. \n",
    "\n",
    "We can expand our problem by specifying more hyperparameters before training, and see how using `dask` as backend can help us. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7805b120",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    'C': [0.001, 0.1, 1.0, 2.5, 5, 10.0],\n",
    "    'kernel': ['rbf', 'poly', 'linear'],\n",
    "    'shrinking': [True, False],\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(estimator, param_grid, verbose=2, cv=2, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0089245",
   "metadata": {},
   "source": [
    "### Dask parallel backend\n",
    "\n",
    "We can fit our estimator with multi-machine parallelism by quickly *switching to a Dask parallel backend* when using joblib. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b678611",
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74c38449",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "with joblib.parallel_backend(\"dask\", scatter=[X, y]):\n",
    "    grid_search.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "097c1a48",
   "metadata": {},
   "source": [
    "**What just happened?**\n",
    "\n",
    "Dask-ML developers worked with the Scikit-Learn and Joblib developers to implement a Dask parallel backend. So internally, scikit-learn now talks to Joblib, and Joblib talks to Dask, and Dask is what handles scheduling all of those tasks on multiple machines.\n",
    "\n",
    "The best parameters and best score:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc5d3872",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search.best_params_, grid_search.best_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18c91175",
   "metadata": {
    "tags": []
   },
   "source": [
    "## But that was cheating...sort of"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64b9305b-f555-488e-862f-d5bde9c8deb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import coiled \n",
    "cluster = coiled.Cluster(\n",
    "    name=\"intro-to-dask\",\n",
    "    n_workers=10,\n",
    "    worker_memory=\"16GiB\",\n",
    "    package_sync=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a607d55-1856-4e08-af21-a4cac02c419f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from distributed import Client\n",
    "client = Client(cluster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db2bdab2",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "with joblib.parallel_backend(\"dask\", scatter=[X, y]):\n",
    "    grid_search.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c66b64d",
   "metadata": {},
   "source": [
    "## Memory Bound: Parallel Machine Learning with Dask-ML\n",
    "\n",
    "We have seen how to work with larger models, but sometimes you'll want to train on a larger than memory dataset. `dask-ml` has implemented estimators that work well on Dask `Arrays` and `DataFrames` that may be larger than your machine's RAM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74bb3918",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask_ml.datasets import make_regression\n",
    "from dask_ml.linear_model import LinearRegression\n",
    "from dask_ml.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24a58ef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create synthetic regression data\n",
    "X, y = make_regression(n_samples=10_000, chunks=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b56cc9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e62aad50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create train/test splits\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=21, test_size=0.3, convert_mixed_types=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e19137a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate model\n",
    "lr = LinearRegression()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcfbb685",
   "metadata": {},
   "source": [
    "### Exercise:\n",
    "Can you fit this parallel Dask-ML `LinearRegression()` model on the training data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44f48412",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/ml-ex-1.py\n",
    "lr.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8b8a1f3",
   "metadata": {},
   "source": [
    "### Exercise:\n",
    "Can you make predictions with this `LinearRegression()` model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4cd999d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/ml-ex-2.py\n",
    "y_pred = lr.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a027213e",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr.score(X,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "630a511a",
   "metadata": {},
   "source": [
    "## Training XGBoost in Parallel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f2e8162",
   "metadata": {},
   "source": [
    "Dask-ML implements some of the most popular machine learning algorithms for parallel processing, but not all of them.\n",
    "\n",
    "For XGBoost, the maintainers of Dask and XGBoost took a different approach: they built a Dask Backend for XGBoost so you can run XGBoost in parallel with Dask straight from your normal XGBoost library.\n",
    "\n",
    "Running an XGBoost model with the distributed Dask backend requires minimal changes to your regular XGBoost code:\n",
    "\n",
    "```python\n",
    "import xgboost as xgb\n",
    "\n",
    "# Create the XGBoost DMatrix for our training and testing splits\n",
    "dtrain = xgb.dask.DaskDMatrix(client, X_train, y_train)\n",
    "dtest = xgb.dask.DaskDMatrix(client, X_test, y_test)\n",
    "\n",
    "# Set model parameters (XGBoost defaults)\n",
    "params = {\n",
    "    \"max_depth\": 6,\n",
    "    \"gamma\": 0,\n",
    "    \"eta\": 0.3,\n",
    "    \"min_child_weight\": 30,\n",
    "    \"objective\": \"reg:squarederror\",\n",
    "    \"grow_policy\": \"depthwise\"\n",
    "}\n",
    "\n",
    "# train the model\n",
    "output = xgb.dask.train(\n",
    "    client, params, dtrain, num_boost_round=4,\n",
    "    evals=[(dtrain, 'train')]\n",
    ")\n",
    "\n",
    "# make predictions\n",
    "y_pred = xgb.dask.predict(client, output, dtest)\n",
    "```\n",
    "\n",
    "See [this step-by-step tutorial](https://coiled.io/blog/dask-xgboost-python-example/) if you're interested to learn more."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e378834",
   "metadata": {},
   "source": [
    "## Extra resources:\n",
    "\n",
    "- [Dask-ML documentation](https://ml.dask.org/)\n",
    "- [Getting started with Coiled](https://docs.coiled.io/user_guide/getting_started.html)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
